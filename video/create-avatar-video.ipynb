{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the TTS Avatar Service to Create Engaging Videos\n",
    "We guide you through creating avatar-based videos using the Azure TTS Avatar API and your own scripts. You can use these videos for various purposes, such as education or communication with customers and partners.\n",
    "\n",
    "First, we'll walk you through creating the script for your avatar using the 'Audio Content Creation' tool. Next, we'll use the Azure TTS Avatar service to turn your script into a video with a custom avatar.\n",
    "\n",
    "We'll then show you how to combine the avatar video with a content image or video created using PowerPoint. To finalize your video, you can use the FFMpeg command line tool for simplicity or the ClipChamp video editor for more advanced options.  \n",
    "\n",
    "__Additional resources:__\n",
    "\n",
    "- Available neural voices and languages: https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=tts\n",
    "- SSML overview: https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-synthesis-markup\n",
    "- Export PowerPoint slides as 1920x1080 resolution PNGs: https://youtu.be/Cv7vGce25rs\n",
    "- FFmpeg command line tool for post processing of videos: https://ffmpeg.org/download.html\n",
    "- Free video editing tool Clipchamp: https://clipchamp.com/\n",
    "\n",
    "__Prerequisites:__  \n",
    "You need an Azure subscription with a Speech resource to use the service. Add the `SPEECH_SERVICEE_REGION` and `SPEECH_SERVICE_API_KEY` to the `.env-avatar-video` file in this folder. You can find the values in the Azure portal under 'Keys and Endpoint' in the Speech resource. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_dotenv('./.env-avatar-video'): raise Exception(\"env file not found\")\n",
    "\n",
    "service_region = os.getenv(\"SPEECH_SERVICEE_REGION\")\n",
    "subscription_key = os.getenv(\"SPEECH_SERVICE_API_KEY\")\n",
    "\n",
    "url_base = f\"https://{service_region}.customvoice.api.speech.microsoft.com/api\"\n",
    "\n",
    "project_folder = './my-project' # your project folder\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "texttype = 'ssml' # ssml or PlainText \n",
    "ssml_path = os.path.join(project_folder, 'ssml.txt') # if your avatar text input is in SSML format\n",
    "plaintext_path = os.path.join(project_folder, 'plaintext.txt') # if your avatar text input is plaintext format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def download_file(url, local_path):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL to a local path. This function streams the file from the URL and writes it in chunks to the local\n",
    "    file system. This allows it to handle large files that might not fit in memory.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the file to download.\n",
    "    local_path (str): The local path where the file should be saved.\n",
    "\n",
    "    Returns:\n",
    "    str: The local path to the downloaded file.\n",
    "    \"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "\n",
    "        # Extract filename from URL\n",
    "        filename = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "\n",
    "        local_filename = os.path.join(local_path, filename)\n",
    "\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide text input for the avatar\n",
    "Your avatar's speaking text can be provided as plain text file `plaintext.txt` or in the Spech Synthesis Markup Language (SSML) format using the file `ssml.txt`. SSML provides more flexibility to adjust neural voices, control pronounciation and add gestures to your avatar. You can use the __Audio Content Generation__ option in the [Speech Studio](https://speech.microsoft.com/portal) to easily generate SSML for various neural voices.\n",
    "\n",
    "<img src=\"./media/audio-content-creation.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "\n",
    "Switch to the SSML view when you are satisfied with the results and copy the SSML content into the `ssml.txt` file of your project folder before you execute the next cell.\n",
    "\n",
    "## Working with SSML directly\n",
    "Alternatvely, you can create or edit your SSML file manually. Here is an example that illustrates a few customization options. Select the `ssml-example` project folder to try it out.\n",
    "\n",
    "```xml\n",
    "<speak xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://www.w3.org/2001/mstts\" xmlns:emo=\"http://www.w3.org/2009/10/emotionml\" version=\"1.0\" xml:lang=\"en-US\">\n",
    "  <voice name=\"en-US-JennyMultilingualNeural\">\n",
    "    This example shows what you can do with SSML to let your avatar speak in various languages and how to add effects. \n",
    "    We are using the voice Jenny Multilangual which is able to speak in several languages.\n",
    "    <lang xml:lang=\"fr-FR\">Voici le talent linguistique Jenny parlant français. Ma voix est bien reconnaissable.</lang>\n",
    "  </voice>\n",
    "  <voice name=\"en-US-JennyNeural\">\n",
    "    I can speak in 15 different styles.\n",
    "    <s />\n",
    "    <mstts:express-as style=\"shouting\">If you want a shouting Avatar, no problem for me!</mstts:express-as>\n",
    "    <s />\n",
    "    <mstts:express-as style=\"whispering\">Or what about some whispered dialog?</mstts:express-as>\n",
    "    You can insert a pause <break strength=\"medium\" /> in my speech with a break tag.\n",
    "    Use a phoneme to pronounce specific words correctly like OpenAI's <phoneme alphabet=\"ipa\" ph=\"ˈdɑli\">DALLE</phoneme> model. Without the phoneme, the neural voice would say DALLE, which is not correct.\n",
    "    Adjust the speaking speed <prosody rate=\"+30.00%\">so I can talk really fast if that's what you want</prosody><prosody rate=\"-40.00%\"> or talk quite slowly.</prosody>\n",
    "    <prosody pitch=\"+10.00%\">Feel free to adjust the pitch for more highness of the sound.</prosody><prosody pitch=\"-20.00%\"> Or the opposite if you like that better.</prosody>\n",
    "    <prosody volume=\"-80.00%\">Finally, here is how you reduce the volume.</prosody>\n",
    "    <prosody volume=\"+40.00%\">Or make me speak louder.</prosody>\n",
    "    We hope that these examples help you to customize your avatar's communication for more engaging experiences.\n",
    "    <s />\n",
    "    <mstts:express-as style=\"friendly\">Have fun!</mstts:express-as>\n",
    "    <s />\n",
    "  </voice>\n",
    "</speak>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load speaking script\n",
    "if texttype == 'ssml':\n",
    "    with open(ssml_path, 'r') as file:\n",
    "        content = file.read()\n",
    "elif texttype == 'Plaintext':\n",
    "    with open(plaintext_path, 'r') as file:\n",
    "        content = file.read()\n",
    "else:\n",
    "    print(f'Error: Texttype needs to be either \"ssml\" or \"Plaintext\". Got {texttype} instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate avatar video\n",
    "You can specify the avatar character, style and further settings below. Note that video synthesis will take a few minutes depending on the length of your talking script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 201\n",
      "Job submitted successfully. Processing ..................................................\n",
      "\n",
      "Ready. Synthesized video:\n",
      "https://cvoiceprodwus2.blob.core.windows.net/batch-synthesis-output/a4e0b1bd-7943-454d-aaff-c43486fd35a3/0001.webm?skoid=85130dbe-2390-4897-a9e9-5c88bb59daff&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2023-11-11T13%3A22%3A41Z&ske=2023-11-17T13%3A27%3A41Z&sks=b&skv=2023-08-03&sv=2023-08-03&st=2023-11-11T13%3A22%3A41Z&se=2023-11-12T13%3A27%3A41Z&sr=b&sp=rl&sig=n7hvXGdawCk6TNrvQQfCUtQXsn5Om4ML6eFsbtNBIa8%3D\n"
     ]
    }
   ],
   "source": [
    "# generate avatar video\n",
    "payload = json.dumps({\n",
    "    \"displayName\": \"my avatar\",\n",
    "    \"description\": \"Vision AI Solution Accelerator Demo\",\n",
    "    \"textType\": texttype, # PlainText, ssml\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"text\": content \n",
    "        }\n",
    "    ],\n",
    "    \"synthesisConfig\": {\n",
    "        \"voice\": \"en-US-JennyNeural\",  # set voice name for plain text; ignored for ssml\n",
    "    },\n",
    "    \"properties\": {\n",
    "        \"talkingAvatarCharacter\": \"lisa\",  # custom avatar. pre-built avatar: lisa\n",
    "        \"talkingAvatarStyle\": \"technical-standing\",  # supported lisa styles: casual-sitting, graceful-sitting, graceful-standing, technical-sitting, technical-standing\n",
    "        \"videoFormat\": \"webm\",  # mp4 or webm, webm is required for transparent background\n",
    "        \"videoCodec\": \"vp9\",  # hevc, h264 or vp9, vp9 is required for transparent background; default is hevc\n",
    "        \"subtitleType\": \"soft_embedded\",\n",
    "        \"backgroundColor\": \"transparent\",\n",
    "    },\n",
    "})\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'Accept': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-Type': 'application/json'\n",
    "})\n",
    "\n",
    "response = session.post(f'{url_base}/texttospeech/3.1-preview1/batchsynthesis/talkingavatar', data=payload)\n",
    "\n",
    "print(f'Status code: {response.status_code}')\n",
    "if response.status_code >= 400:\n",
    "    print('Job submission failed. Please verify your subscription key and try again.')\n",
    "    print(response.text)\n",
    "else:\n",
    "    print('Job submitted successfully. Processing', end=' ')\n",
    "    r = response.json()\n",
    "\n",
    "    while True:\n",
    "        result = session.get(f'{url_base}/texttospeech/3.1-preview1/batchsynthesis/talkingavatar/{r[\"id\"]}')\n",
    "        if result.json()['status'] == 'Succeeded':\n",
    "            print('\\n\\nReady. Synthesized video:\\n' + result.json()['outputs']['result'])\n",
    "            break\n",
    "        if result.json()['status'] == 'Failed':\n",
    "            print('synthesis failed')\n",
    "            print(result.json()['properties']['error'])\n",
    "            break\n",
    "        print('.', end = '')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./my-project/0001.webm downloaded.\n"
     ]
    }
   ],
   "source": [
    "# download video to project folder. Note: An existing video file will be overwritten.\n",
    "url = result.json()['outputs']['result']\n",
    "local_filename = download_file(url, project_folder)\n",
    "print(f'{local_filename} downloaded.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add content to your avatar video\n",
    "In addition to the avatar video, we require another asset to represent the content we wish to display alongside the avatar. This could be either high-resolution images or a separate video, depending on your specific needs. \n",
    "\n",
    "<img src=\"./media/content-avatar.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "\n",
    "PowerPoint is an excellent tool for creating this supplemental content, and here's how you can do it:\n",
    "\n",
    "1. __Export slides as high-resolution images__: PowerPoint allows you to export slides as images. For optimal results, we recommend exporting the slides as 1920x1080 PNG files. You can follow this brief tutorial on how to do so: [Export slides as 1920x1080 PNGs](https://youtu.be/Cv7vGce25rs).\n",
    "\n",
    "2. Alternatively __export the entire presentation as an MP4 video__: If you prefer to use a video instead of static images, PowerPoint offers the option to export your whole animated presentation as an MP4 file.  Here's how you can do it:  \n",
    "Go to __File__ > __Export__ > __Create a Video__ in PowerPoint. Use slide transition and animation durations to adjust timings. The best way to align avatar and content video timing is to use a video editor (Option 2)\n",
    "\n",
    "> **Tip:** For smooth synchronization, match the PowerPoint slide transition durations with the timestamps from your avatar video. This preemptive alignment minimizes the need for later timing adjustments.\n",
    "\n",
    "Lastly, there are various methods available for integrating the avatar video and the content assets to produce your final video output.\n",
    "\n",
    "### Option 1: Use the FFmpeg command line \n",
    "\n",
    "Install FFmpeg on a Linux system (Check out the [FFMpeg website](https://ffmpeg.org/download.html) for Windows and Mac options):\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install ffmpeg\n",
    "```\n",
    "#### Examples\n",
    "Start your terminal and navigate to the project folder.\n",
    "\n",
    ">Add a content image as background to transparent avatar video:\n",
    ">\n",
    ">```bash\n",
    ">ffmpeg -i content.png -vcodec libvpx-vp9 -i 0001.webm -filter_complex \"overlay=(main_w-overlay_w)/2:(main_h-overlay_h)/2\" -map 1:a output.mp4\n",
    ">```\n",
    "\n",
    ">Add a content video as background to transparent avatar video:\n",
    ">\n",
    ">```bash\n",
    ">ffmpeg -i content.mp4 -vcodec libvpx-vp9 -i 0001.webm -filter_complex \"overlay=(main_w-overlay_w)/2:(main_h-overlay_h)/2\" -map 1:a output.mp4\n",
    ">```\n",
    "\n",
    ">Add a content video and an audio background music file with reduced volume:\n",
    ">```bash\n",
    ">ffmpeg -i content.mp4 -vcodec libvpx-vp9 -i 0001.webm -i background.wav -filter_complex \"[2:a]volume=0.3[bg]; [1:a][bg]amix=inputs=2:duration=first[a]; overlay=(main_w-overlay_w)/2:(main_h-overlay_h)/2[v]\" -map \"[v]\" -map \"[a]\" output.mp4\n",
    ">```\n",
    "\n",
    ">Crop avatar and move it to the right:\n",
    ">```bash\n",
    ">ffmpeg -i content.mp4 -vcodec libvpx-vp9 -i 0001.webm -filter_complex \"[1:v]crop=440:1042:740:38[webm];[0:v][webm]overlay=W-w-160:38[outv]\" -map \"[outv]\" -map 1:a output.mp4\n",
    ">```\n",
    ">Above command was adjusted to the default size of the Lisa avatar in \"technical standing\" style. The general pattern of croping and repositioning is as follows:\n",
    ">\n",
    ">```bash\n",
    ">ffmpeg -i content.mp4 -vcodec libvpx-vp9 -i 0001.webm -filter_complex \"[1:v]crop=w:h:x:y[webm];[0:v][webm]overlay=W-w-10:10[outv]\" -map \"[outv]\" -map 1:a output.mp4\n",
    ">```\n",
    ">- __[1:v]crop=w:h:x:y[webm]__ is the cropping filter. Replace w, h, x, and y with the width, height, and the x, y coordinates of the top-left corner of the crop rectangle.\n",
    ">- __[0:v][webm]overlay=W-w-10:10[outv]__ is the overlay filter. The overlay is positioned 10 pixels from the right edge and 10 pixels from the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Option 2: Use a video editing tool\n",
    "\n",
    "Using a video editing tool provides more intuitive and flexible options for generating your final output video. The following screenshot illustrates how to use the free edition of Microsoft Clipchamp.\n",
    "\n",
    "<img src=\"./media/Clipchamp.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "\n",
    "You can multi-select the required files in Windows Explorer and select \"Edit with Clipchamp\" in the context menu. In our example, we have selected the avatar video `0001.webm`, the content video `content.mp4`, and an audio file for background music `background.wav`.\n",
    "\n",
    "Ensure that you add the content as tracks in the video editor, positioning the avatar track with the transparent background on top. Then, align the timing of your tracks using the video cutting option. You can also use the tool to reposition your avatar, generate captions for subtitles, and add further content and special effects.\n",
    "Lastly, use the export button to create your final video.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
